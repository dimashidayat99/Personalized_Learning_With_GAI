# Generative Adversarial Network (GAN)

# Data Preparation

The GAN model take only user entity data as input. To obtained the historical course sequences from user entity, the data was prepared based on several steps namely data
sampling, feature selection, data transformation, data encoding and data splitting.

1. Sampling: The preparation begins with data sampling where only consider five to ten courses in the sequence and 30% of the data was selected as a sample. The filtering and sampling the
data in such way help to reduced the huge number volume and size of data which help
to reduce complexity of data pre-processing and the modelling. The consequences of
sampling process will make the sample data have different number of available courses
where initially there are 706 courses in dataset is now reduced to 660 courses in the sample data.

2. Feature Selection & Data Transformation: Next, the specific feature was selected manually where in this case, the selected features was the course order feature (course sequences). The length course sequence will be vary from five to ten courses, in order to make a new dataframe, the zero padding technique was applied to make sure the length of the course sequence will be the same in for all instances. Zero padding is a technique to append zero in the in the data to obtained fixed length data dimension. In this study, the zero was left padded into the data, so that the final course in the sequence will be in the final position of the sequence. The fixed size of the input is necessary because the artificial neural network require fixed size of input data. The zero padding technique was used to avoid over complicated of the modelling process.  Using zero padding technique, noted that zero value will become one of course value which represent "empty" course. Therefore, the unique value of the course will become 661 courses.

3. Data Encoding: After that, the zero padded data was encoded using label encoder where the course ID will be transform into numerical value. The data was encoded second time by using one hot encoding for better data representation. Then, the data was split into X and Y component where X component will be courses sequence from the first course until penultimate course in the course sequences and Y is the final course in the course sequence.

4. Data Splitting: Finally, the data was split into train and test data with 80% of the data will become train data while 20% of the data will become test data. Overall, the final result of data preparation is one hot encoded course sequence data for ten course sequences for X and Y component for training and testing data. Where X component will have 5949 features and Y component will have 661 features which represent one hot encoded data for nine courses sequences and one hot encoded data final courses.

# Modelling
The conventional GAN model contained two main component which are generator
component and discriminator component. Each component will have different architectures
of neural network. 

1. Generator: The generator component will take the X component of training data of historical course sequences as input. The generator component consist of four layers namely which are one input layer, two hidden layers and one output layer (predictive layer). The neurons of input layer will follow the input size of training data. In this case, the input layer will have 5949 neurons. The hidden layer was created using dense layer where the first hidden layer consist of 50 neurons and second hidden layer consist of 25 neurons. Both hidden layer will use Rectified Linear Unit (ReLU) as activation function. The small number of neuron is selected to avoid high training time of the model since there are total of six models will be created. The output layer was created using dense layer which have 661 neurons with softmax activation function. Softmax activation function will provide output of generator to a probability distribution over all courses. At this point, the generator output which is the predicted course can be seen as probability distribution of courses. The generator model was compiled with adam optimizer and the loss metric of mean absolute error. The output from the generator was combined together with the X component of training data of historical sequence forming generated courses sequences. On the other hand, the X component of training data and Y component of training data also will be combined together forming real courses sequences. Both generated courses sequences and real courses sequences will become input in the discriminator component.

2. Discriminator: The discriminator network also consist of four layer which are one input layer, two hidden layer and one output layer. The input layer will have 6610 neurons since the input of the discriminators is the complete courses sequences from combined courses sequences as mentioned before. The hidden layer was created based on the dense layer where the first hidden layer consist of 25 neurons and second hidden layer consist of 50 neurons. Both hidden layer will use ReLU activation function. The similar reason is used for having small number of neurons which is to avoid high training time. The output layer was created using dense layer which have one neuron with sigmoid activation function. Using sigmoid activation function, the output from discriminator will return any floating-point number to predict the probability of a binary variables which in this case, the binary variables are real or fake. Both generated courses sequences and real courses sequences will be input of the discriminator and the discriminator will discriminate whether the generated courses sequences are real or fake. The discriminator model will be compile with adam optimizer, loss function of binary crossentropy and metrics of mean absolute error.

Both generator and discriminator network was combine together forming GAN model. Noted that, the output from generator require to be integrated with X component of the training data. Therefore, a preprocessing layer must be created in between generator network and discriminator network. The pre-processing layer is created by using Lambda layer which help to concatenate the X component of training data and generated data from generator output. The GAN model was compiled with adam optimizer and loss function of mean absolute error. Both component of GAN which are generator and discriminator was trained alternatively for 50 epoch. Overall process of the GAN model is start from generator network take input from X component of one encoded courses sequences training data which produce generated probability distribution of one hot encoded final course. The X component of one hot encoded courses sequences training data will be combined together with generated one hot encoded final course to form the generated courses sequences. On the other hand, the X component of one encoded courses sequences training data will be combined together with Y component of one hot encoded final course training data to form real courses sequences. Then, the discriminator network will take both generated and real courses sequences data to become the input. The discriminator will evaluates the rationality of the generated courses sequences based on real courses sequences. The evaluation results will be sent back to the generator to escort and lead the learning of following round. While, discriminator will be updated by improving the discriminative ability and capacity based on comparison of generated courses sequences and real courses sequences. In this case, the generator and multiple discriminators will play the min-max game where the generator and multiple discriminator compete with each other to improve and outperform each other in a mutual reinforcement way. After training is completed, the X component of courses sequences of test data will be input to the GAN model to predict the top five courses with highest probability score. The Y component of actual final course and top five predicted courses with highest probability score will be evaluated based on the performances metrics.


